<<<<<<< HEAD
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["tXgIY1zCsLuz","516fbc79"],"authorship_tag":"ABX9TyNZI27TE0WJGmeqht2a+3NB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### IMPORTING LIBRARIES"],"metadata":{"id":"KBB5hXGmiQrD"}},{"cell_type":"code","source":["from google.colab import files\n","files.upload()"],"metadata":{"id":"Wz3muYudaTK6","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1754039228968,"user_tz":180,"elapsed":6083,"user":{"displayName":"Pedro Isaia Alves de Souza","userId":"02292677914546630569"}},"outputId":"1887349a-ce93-446c-e0a9-c08d3b6dbd7a"},"execution_count":51,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-45048ecc-74e3-45fd-a3eb-d3dc1078463a\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-45048ecc-74e3-45fd-a3eb-d3dc1078463a\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving future_engineering.ipynb to future_engineering.ipynb\n","Saving modeling.ipynb to modeling.ipynb\n"]},{"output_type":"execute_result","data":{"text/plain":["{'future_engineering.ipynb': b'{\\n  \"nbformat\": 4,\\n  \"nbformat_minor\": 0,\\n  \"metadata\": {\\n    \"colab\": {\\n      \"provenance\": [],\\n      \"collapsed_sections\": [\\n        \"LAaEydDjhY2T\",\\n        \"gRDtn0XbgtUa\",\\n        \"e3a24b8f\"\\n      ]\\n    },\\n    \"kernelspec\": {\\n      \"name\": \"python3\",\\n      \"display_name\": \"Python 3\"\\n    },\\n    \"language_info\": {\\n      \"name\": \"python\"\\n    }\\n  },\\n  \"cells\": [\\n    {\\n      \"cell_type\": \"markdown\",\\n      \"source\": [\\n        \"### IMPORTING LIBRARIES AND LOADING DATASETS\"\\n      ],\\n      \"metadata\": {\\n        \"id\": \"LAaEydDjhY2T\"\\n      }\\n    },\\n    {\\n      \"cell_type\": \"code\",\\n      \"execution_count\": 21,\\n      \"metadata\": {\\n        \"id\": \"jONgyXZ5Ywch\"\\n      },\\n      \"outputs\": [],\\n      \"source\": [\\n        \"import warnings\\\\n\",\\n        \"warnings.filterwarnings(\\'ignore\\')\"\\n      ]\\n    },\\n    {\\n      \"cell_type\": \"code\",\\n      \"source\": [\\n        \"#Exploratory Data Analysis Libraries\\\\n\",\\n        \"import pandas as pd\\\\n\",\\n        \"import seaborn as sns\\\\n\",\\n        \"import numpy as np\\\\n\",\\n        \"import matplotlib.pyplot as plt\"\\n      ],\\n      \"metadata\": {\\n        \"id\": \"2r8QpYmRaTpx\"\\n      },\\n      \"execution_count\": 22,\\n      \"outputs\": []\\n    },\\n    {\\n      \"cell_type\": \"markdown\",\\n      \"source\": [\\n        \"### MERGING EXTRA FILES\"\\n      ],\\n      \"metadata\": {\\n        \"id\": \"gRDtn0XbgtUa\"\\n      }\\n    },\\n    {\\n      \"cell_type\": \"code\",\\n      \"source\": [\\n        \"def merge_dataframe(train, test, oil, holidays, transactions, stores):\\\\n\",\\n        \"\\\\n\",\\n        \"  if \\'SalePrice\\' not in test.columns:\\\\n\",\\n        \"    test[\\'SalePrice\\'] = 0\\\\n\",\\n        \"\\\\n\",\\n        \"  df = pd.concat([train, test], axis = 0)\\\\n\",\\n        \"  df = df.set_index(\\'id\\')\\\\n\",\\n        \"\\\\n\",\\n        \"  # --- Merge Oil Prices ---\\\\n\",\\n        \"  oil[\\'dcoilwtico\\'] = oil[\\'dcoilwtico\\'].ffill()\\\\n\",\\n        \"  df = df.merge(oil, on=\\'date\\', how=\\'left\\')\\\\n\",\\n        \"\\\\n\",\\n        \"  # --- Merge Holidays ---\\\\n\",\\n        \"  holidays = df[[\\'date\\', \\'holiday\\']].drop_duplicates(subset=\\'date\\')\\\\n\",\\n        \"  holidays[\\'is_holiday\\'] = 1\\\\n\",\\n        \"  holidays = holidays[[\\'date\\', \\'is_holiday\\']].drop_duplicates(subset=\\'date\\')\\\\n\",\\n        \"  df = df.merge(holidays, on=\\'date\\', how=\\'left\\')\\\\n\",\\n        \"  df[\\'is_holiday\\'] = df[\\'is_holiday\\'].fillna(0)\\\\n\",\\n        \"\\\\n\",\\n        \"  # --- Merge Transactions ---\\\\n\",\\n        \"  transactions = df[[\\'date\\', \\'store_nbr\\', \\'transactions\\']].drop_duplicates(subset=[\\'date\\', \\'store_nbr\\'])\\\\n\",\\n        \"  df = df.merge(transactions, on=[\\'date\\', \\'store_nbr\\'], how=\\'left\\')\\\\n\",\\n        \"  df[\\'transactions\\'] = df[\\'transactions\\'].fillna(0)\\\\n\",\\n        \"\\\\n\",\\n        \"  # --- Merge Store Metadata ---\\\\n\",\\n        \"  stores = df[[\\'store_nbr\\', \\'city\\', \\'state\\', \\'type\\']].drop_duplicates(subset=\\'store_nbr\\')\\\\n\",\\n        \"  df = df.merge(stores, on=\\'store_nbr\\', how=\\'left\\')\\\\n\",\\n        \"\\\\n\",\\n        \"  print(\\\\\"Dataset shape after merges:\\\\\", df.shape)\\\\n\",\\n        \"\\\\n\",\\n        \"  return df\"\\n      ],\\n      \"metadata\": {\\n        \"id\": \"AZjrxvSdphxi\"\\n      },\\n      \"execution_count\": 23,\\n      \"outputs\": []\\n    },\\n    {\\n      \"cell_type\": \"markdown\",\\n      \"metadata\": {\\n        \"id\": \"e3a24b8f\"\\n      },\\n      \"source\": [\\n        \"### FUTURE ENGINEERING\"\\n      ]\\n    },\\n    {\\n      \"cell_type\": \"code\",\\n      \"execution_count\": 24,\\n      \"metadata\": {\\n        \"id\": \"f0d7e1bc\"\\n      },\\n      \"outputs\": [],\\n      \"source\": [\\n        \"def future_engineering(df):\\\\n\",\\n        \"\\\\n\",\\n        \"  #Convert categorical column (family) to numeric codes\\\\n\",\\n        \"  df[\\'family_encoded\\'] = df[\\'family\\'].astype(\\'category\\').cat.codes\\\\n\",\\n        \"\\\\n\",\\n        \"  # Encode object columns to numeric\\\\n\",\\n        \"  for col in [\\'city\\', \\'state\\', \\'type\\']:\\\\n\",\\n        \"    if col in df.columns:\\\\n\",\\n        \"      df[col] = df[col].astype(\\'category\\').cat.codes\\\\n\",\\n        \"\\\\n\",\\n        \"  #Add date-based feature\\\\n\",\\n        \"  df[\\'day_of_week\\'] = df[\\'date\\'].dt.dayofweek\\\\n\",\\n        \"  df[\\'month\\'] = df[\\'date\\'].dt.month\\\\n\",\\n        \"  df[\\'year\\'] = df[\\'date\\'].dt.year\\\\n\",\\n        \"  df[\\'day_of_month\\'] = df[\\'date\\'].dt.day\\\\n\",\\n        \"  df[\\'week_of_year\\'] = df[\\'date\\'].dt.isocalendar().week.astype(int)\\\\n\",\\n        \"\\\\n\",\\n        \"  #Weekend Flag\\\\n\",\\n        \"  df[\\'is_weekend\\'] = df[\\'day_of_week\\'].isin([5, 6]).astype(int)\\\\n\",\\n        \"\\\\n\",\\n        \"  #Store type encoding\\\\n\",\\n        \"  df[\\'store_type_encoded\\'] = df[\\'type\\'].astype(\\'category\\').cat.codes\\\\n\",\\n        \"\\\\n\",\\n        \"  #Holiday Interaction\\\\n\",\\n        \"  # Higher sales expected just before holidays\\\\n\",\\n        \"  df[\\'holiday_lag\\'] = df.groupby(\\'store_nbr\\')[\\'is_holiday\\'].shift(-1).fillna(0)\\\\n\",\\n        \"\\\\n\",\\n        \"  return df\"\\n      ]\\n    }\\n  ]\\n}',\n"," 'modeling.ipynb': b'{\\n  \"nbformat\": 4,\\n  \"nbformat_minor\": 0,\\n  \"metadata\": {\\n    \"colab\": {\\n      \"provenance\": [],\\n      \"collapsed_sections\": [\\n        \"SESTwB1ciKfE\",\\n        \"f5dd591c\",\\n        \"499312ed\"\\n      ]\\n    },\\n    \"kernelspec\": {\\n      \"name\": \"python3\",\\n      \"display_name\": \"Python 3\"\\n    },\\n    \"language_info\": {\\n      \"name\": \"python\"\\n    }\\n  },\\n  \"cells\": [\\n    {\\n      \"cell_type\": \"markdown\",\\n      \"source\": [\\n        \"### IMPORTING LIBRARIES\"\\n      ],\\n      \"metadata\": {\\n        \"id\": \"SESTwB1ciKfE\"\\n      }\\n    },\\n    {\\n      \"cell_type\": \"code\",\\n      \"source\": [\\n        \"import warnings\\\\n\",\\n        \"warnings.filterwarnings(\\'ignore\\')\"\\n      ],\\n      \"metadata\": {\\n        \"id\": \"YLEfV6I0aR4o\"\\n      },\\n      \"execution_count\": 10,\\n      \"outputs\": []\\n    },\\n    {\\n      \"cell_type\": \"code\",\\n      \"source\": [\\n        \"#Exploratory Data Analysis Libraries\\\\n\",\\n        \"import pandas as pd\\\\n\",\\n        \"import seaborn as sns\\\\n\",\\n        \"import numpy as np\\\\n\",\\n        \"import matplotlib.pyplot as plt\"\\n      ],\\n      \"metadata\": {\\n        \"id\": \"fC6RcUzwlnCk\"\\n      },\\n      \"execution_count\": 11,\\n      \"outputs\": []\\n    },\\n    {\\n      \"cell_type\": \"code\",\\n      \"execution_count\": 12,\\n      \"metadata\": {\\n        \"id\": \"eTKEL1bNY42H\",\\n        \"colab\": {\\n          \"base_uri\": \"https://localhost:8080/\"\\n        },\\n        \"outputId\": \"a9c7240b-561c-42f9-9ea5-85aa27bb0488\"\\n      },\\n      \"outputs\": [\\n        {\\n          \"output_type\": \"stream\",\\n          \"name\": \"stdout\",\\n          \"text\": [\\n            \"Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.4.0)\\\\n\",\\n            \"Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.4)\\\\n\",\\n            \"Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\\\\n\",\\n            \"Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\\\\n\",\\n            \"Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\\\\n\",\\n            \"Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.42)\\\\n\",\\n            \"Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\\\\n\",\\n            \"Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\\\\n\",\\n            \"Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\\\\n\",\\n            \"Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\\\\n\",\\n            \"Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\\\\n\"\\n          ]\\n        }\\n      ],\\n      \"source\": [\\n        \"!pip install optuna\\\\n\",\\n        \"#Predictive Models\\\\n\",\\n        \"from sklearn.preprocessing import OrdinalEncoder\\\\n\",\\n        \"from sklearn.preprocessing import MinMaxScaler\\\\n\",\\n        \"from sklearn.preprocessing import OneHotEncoder\\\\n\",\\n        \"from sklearn.preprocessing import StandardScaler\\\\n\",\\n        \"from sklearn.ensemble import IsolationForest\\\\n\",\\n        \"from sklearn.pipeline import make_pipeline\\\\n\",\\n        \"import lightgbm as lgb\\\\n\",\\n        \"from sklearn.metrics import mean_squared_log_error\\\\n\",\\n        \"import xgboost as xgb\\\\n\",\\n        \"from xgboost import plot_importance\\\\n\",\\n        \"import optuna\\\\n\",\\n        \"from sklearn.model_selection import train_test_split\"\\n      ]\\n    },\\n    {\\n      \"cell_type\": \"markdown\",\\n      \"metadata\": {\\n        \"id\": \"f5dd591c\"\\n      },\\n      \"source\": [\\n        \"### PREPARING DATA FOR PROCESSING\"\\n      ]\\n    },\\n    {\\n      \"cell_type\": \"code\",\\n      \"source\": [\\n        \"def separate_train(df):\\\\n\",\\n        \"\\\\n\",\\n        \"  #Separate train and test data\\\\n\",\\n        \"  df_dummies = pd.get_dummies(df, drop_first=True)\\\\n\",\\n        \"\\\\n\",\\n        \"  train_start = \\'2015-01-01\\'\\\\n\",\\n        \"  train_end = \\'2017-08-15\\'\\\\n\",\\n        \"\\\\n\",\\n        \"  train_data = df_dummies[(df_dummies[\\'date\\'] >= train_start) & (df_dummies[\\'date\\'] <= train_end)]\\\\n\",\\n        \"\\\\n\",\\n        \"  feature_columns = [col for col in df_dummies.columns if col not in [\\'sales\\', \\'date\\']]\\\\n\",\\n        \"\\\\n\",\\n        \"  return train_data\"\\n      ],\\n      \"metadata\": {\\n        \"id\": \"D3Idrh4clQV1\"\\n      },\\n      \"execution_count\": 13,\\n      \"outputs\": []\\n    },\\n    {\\n      \"cell_type\": \"code\",\\n      \"source\": [\\n        \"def separate_test(df):\\\\n\",\\n        \"\\\\n\",\\n        \"  #Separate train and test data\\\\n\",\\n        \"  df_dummies = pd.get_dummies(df, drop_first=True)\\\\n\",\\n        \"\\\\n\",\\n        \"  test_start = \\'2017-08-16\\'\\\\n\",\\n        \"  test_end = \\'2017-08-31\\'\\\\n\",\\n        \"\\\\n\",\\n        \"  test_data = df_dummies[(df_dummies[\\'date\\'] >= test_start) & (df_dummies[\\'date\\'] <= test_end)]\\\\n\",\\n        \"\\\\n\",\\n        \"  return test_data\"\\n      ],\\n      \"metadata\": {\\n        \"id\": \"GSXcuBM8s-bH\"\\n      },\\n      \"execution_count\": 14,\\n      \"outputs\": []\\n    },\\n    {\\n      \"cell_type\": \"code\",\\n      \"execution_count\": 15,\\n      \"metadata\": {\\n        \"id\": \"c76fff0c\"\\n      },\\n      \"outputs\": [],\\n      \"source\": [\\n        \"def prepare_data(df):\\\\n\",\\n        \"\\\\n\",\\n        \"  #Import data\\\\n\",\\n        \"  train = separate_train(df)\\\\n\",\\n        \"  test = separate_test(df)\\\\n\",\\n        \"\\\\n\",\\n        \"  #Prepare Features and Target\\\\n\",\\n        \"  # Target variable\\\\n\",\\n        \"  y = train[\\'sales\\']\\\\n\",\\n        \"\\\\n\",\\n        \"  # Features to use (add/remove as needed)\\\\n\",\\n        \"  # Drop columns not used as features\\\\n\",\\n        \"  drop_cols = [\\'id\\', \\'date\\', \\'sales\\', \\'family\\']  # keep family_encoded\\\\n\",\\n        \"\\\\n\",\\n        \"  # Select all other columns as features\\\\n\",\\n        \"  features = [col for col in train.columns if col not in drop_cols]\\\\n\",\\n        \"\\\\n\",\\n        \"  # Prepare training and test data\\\\n\",\\n        \"  X = train[features]\\\\n\",\\n        \"  X_test = test[features]\\\\n\",\\n        \"\\\\n\",\\n        \"  return X, y, X_test\"\\n      ]\\n    },\\n    {\\n      \"cell_type\": \"markdown\",\\n      \"metadata\": {\\n        \"id\": \"499312ed\"\\n      },\\n      \"source\": [\\n        \"### TRAINING MODEL\"\\n      ]\\n    },\\n    {\\n      \"cell_type\": \"code\",\\n      \"execution_count\": 16,\\n      \"metadata\": {\\n        \"id\": \"0cfaa32e\"\\n      },\\n      \"outputs\": [],\\n      \"source\": [\\n        \"def train_model(df):\\\\n\",\\n        \"\\\\n\",\\n        \"  #Import data\\\\n\",\\n        \"  X, y, X_test = prepare_data(df)\\\\n\",\\n        \"\\\\n\",\\n        \"  #Train/Test Split\\\\n\",\\n        \"  X_train, X_val, y_train, y_val = train_test_split(\\\\n\",\\n        \"\\\\n\",\\n        \"      X, y, test_size=0.2, shuffle=False\\\\n\",\\n        \"\\\\n\",\\n        \"      )  # No shuffling due to time-series nature\\\\n\",\\n        \"\\\\n\",\\n        \"    #Train XGBoost Model\\\\n\",\\n        \"  model = xgb.XGBRegressor(\\\\n\",\\n        \"    n_estimators=300,\\\\n\",\\n        \"    learning_rate=0.1,\\\\n\",\\n        \"    max_depth=6,\\\\n\",\\n        \"    subsample=0.8,\\\\n\",\\n        \"    colsample_bytree=0.8,\\\\n\",\\n        \"    random_state=42\\\\n\",\\n        \"  )\\\\n\",\\n        \"\\\\n\",\\n        \"  model.fit(X_train, y_train)\\\\n\",\\n        \"\\\\n\",\\n        \"  return model\"\\n      ]\\n    }\\n  ]\\n}'}"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["from future_engineering import merge_dataframe, future_engineering\n","from modeling import train_model"],"metadata":{"id":"Bv9_JGjm0v8k","executionInfo":{"status":"ok","timestamp":1754039228972,"user_tz":180,"elapsed":6,"user":{"displayName":"Pedro Isaia Alves de Souza","userId":"02292677914546630569"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","execution_count":53,"metadata":{"id":"G4J-I6sKZAwg","executionInfo":{"status":"ok","timestamp":1754039228993,"user_tz":180,"elapsed":3,"user":{"displayName":"Pedro Isaia Alves de Souza","userId":"02292677914546630569"}}},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["#Exploratory Data Analysis Libraries\n","import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"5v4FBOQYaTPC","executionInfo":{"status":"ok","timestamp":1754039228999,"user_tz":180,"elapsed":4,"user":{"displayName":"Pedro Isaia Alves de Souza","userId":"02292677914546630569"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["#Predictive Models\n","from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import IsolationForest\n","from sklearn.pipeline import make_pipeline\n","import lightgbm as lgb\n","from sklearn.metrics import mean_squared_log_error\n","import xgboost as xgb\n","from xgboost import plot_importance\n","import optuna"],"metadata":{"id":"0F4J6dH4iFw5","executionInfo":{"status":"ok","timestamp":1754039229003,"user_tz":180,"elapsed":6,"user":{"displayName":"Pedro Isaia Alves de Souza","userId":"02292677914546630569"}}},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":["### IMPORTING DATA AND RUNNING FUNCTIONS"],"metadata":{"id":"tXgIY1zCsLuz"}},{"cell_type":"code","source":["train = pd.read_csv('train.csv', parse_dates=['date'])\n","test = pd.read_csv('test.csv', parse_dates=['date'])\n","oil = pd.read_csv('oil.csv', parse_dates=['date'])\n","holidays = pd.read_csv('holidays_events.csv', parse_dates=['date'])\n","transactions = pd.read_csv('transactions.csv', parse_dates=['date'])\n","stores = pd.read_csv('stores.csv')"],"metadata":{"id":"cb6_Nv61sLnn","executionInfo":{"status":"ok","timestamp":1754039229997,"user_tz":180,"elapsed":992,"user":{"displayName":"Pedro Isaia Alves de Souza","userId":"02292677914546630569"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["df = merge_dataframe(train, test, oil, holidays, transactions, stores)"],"metadata":{"id":"P-_NS7T9sXuO","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"error","timestamp":1754039230023,"user_tz":180,"elapsed":19,"user":{"displayName":"Pedro Isaia Alves de Souza","userId":"02292677914546630569"}},"outputId":"b9dfe7c2-fd13-425d-abc4-3de576964280"},"execution_count":57,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"merge_dataframe() takes 2 positional arguments but 6 were given","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3406897397.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholidays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: merge_dataframe() takes 2 positional arguments but 6 were given"]}]},{"cell_type":"code","source":["df = future_engineering(df)"],"metadata":{"id":"WEnvzlmFsX1I","executionInfo":{"status":"aborted","timestamp":1754039230029,"user_tz":180,"elapsed":1,"user":{"displayName":"Pedro Isaia Alves de Souza","userId":"02292677914546630569"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = train_model(df)"],"metadata":{"id":"TO7KmOHrwqsi","executionInfo":{"status":"aborted","timestamp":1754039230032,"user_tz":180,"elapsed":7319,"user":{"displayName":"Pedro Isaia Alves de Souza","userId":"02292677914546630569"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"516fbc79"},"source":["### SUBMISSION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dab1d08","executionInfo":{"status":"aborted","timestamp":1754039230035,"user_tz":180,"elapsed":7322,"user":{"displayName":"Pedro Isaia Alves de Souza","userId":"02292677914546630569"}}},"outputs":[],"source":["# Validation performance\n","val_pred = model.predict(X_val)\n","rmsle = np.sqrt(mean_squared_log_error(y_val, np.maximum(0, val_pred)))\n","print(\"Validation RMSLE:\", rmsle)\n","\n","#Generate Submission\n","# Predict on test set\n","test['sales'] = np.maximum(0, model.predict(X_test))\n","\n","# Create submission\n","submission = test[['id', 'sales']]\n","submission.to_csv('submission.csv', index=False)"]},{"cell_type":"code","metadata":{"id":"UPVDd406aTtk","executionInfo":{"status":"aborted","timestamp":1754039230039,"user_tz":180,"elapsed":2,"user":{"displayName":"Pedro Isaia Alves de Souza","userId":"02292677914546630569"}}},"source":["def merge_features(df, oil, holidays, transactions, stores):\n","\n","  # --- Merge Oil Prices ---\n","  oil['dcoilwtico'] = oil['dcoilwtico'].ffill()\n","  df = df.merge(oil, on='date', how='left')\n","\n","  # --- Merge Holidays ---\n","  # Ensure 'date' column in holidays is datetime type before merging\n","  holidays['date'] = pd.to_datetime(holidays['date'])\n","  # Filter holidays to include only those present in the main dataframe's date range\n","  holidays_filtered = holidays[holidays['date'].isin(df['date'].unique())]\n","  # Create a column 'is_holiday' to indicate if a date is a holiday\n","  holidays_filtered['is_holiday'] = 1\n","  # Select only 'date' and 'is_holiday' columns and drop duplicates based on 'date'\n","  holidays_processed = holidays_filtered[['date', 'is_holiday']].drop_duplicates(subset='date')\n","  # Merge with the main dataframe\n","  df = df.merge(holidays_processed, on='date', how='left')\n","  # Fill NaN values in 'is_holiday' with 0, indicating non-holidays\n","  df['is_holiday'] = df['is_holiday'].fillna(0)\n","\n","  # --- Merge Transactions ---\n","  # Ensure 'date' column in transactions is datetime type before merging\n","  transactions['date'] = pd.to_datetime(transactions['date'])\n","  # Filter transactions to include only those present in the main dataframe's date and store_nbr combinations\n","  transactions_filtered = transactions[transactions.set_index(['date', 'store_nbr']).index.isin(df.set_index(['date', 'store_nbr']).index)]\n","  # Select only 'date', 'store_nbr', and 'transactions' columns and drop duplicates\n","  transactions_processed = transactions_filtered[['date', 'store_nbr', 'transactions']].drop_duplicates(subset=['date', 'store_nbr'])\n","  # Merge with the main dataframe\n","  df = df.merge(transactions_processed, on=['date', 'store_nbr'], how='left')\n","  # Fill NaN values in 'transactions' with 0\n","  df['transactions'] = df['transactions'].fillna(0)\n","\n","\n","  # --- Merge Store Metadata ---\n","  # Select only 'store_nbr', 'city', 'state', and 'type' columns from the stores dataframe\n","  stores_processed = stores[['store_nbr', 'city', 'state', 'type']].drop_duplicates(subset='store_nbr')\n","  # Merge with the main dataframe\n","  df = df.merge(stores_processed, on='store_nbr', how='left')\n","\n","\n","  print(\"Dataset shape after merges:\", df.shape)\n","\n","  return df"],"execution_count":null,"outputs":[]}]}
=======
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBB5hXGmiQrD"
   },
   "source": [
    "### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6083,
     "status": "ok",
     "timestamp": 1754039228968,
     "user": {
      "displayName": "Pedro Isaia Alves de Souza",
      "userId": "02292677914546630569"
     },
     "user_tz": 180
    },
    "id": "Wz3muYudaTK6",
    "outputId": "1887349a-ce93-446c-e0a9-c08d3b6dbd7a"
   },
   "outputs": [],
   "source": [
    "from future_engineering import merge_dataframe, future_engineering\n",
    "\n",
    "from modeling import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1754039228993,
     "user": {
      "displayName": "Pedro Isaia Alves de Souza",
      "userId": "02292677914546630569"
     },
     "user_tz": 180
    },
    "id": "G4J-I6sKZAwg"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1754039228999,
     "user": {
      "displayName": "Pedro Isaia Alves de Souza",
      "userId": "02292677914546630569"
     },
     "user_tz": 180
    },
    "id": "5v4FBOQYaTPC"
   },
   "outputs": [],
   "source": [
    "#Exploratory Data Analysis Libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1754039229003,
     "user": {
      "displayName": "Pedro Isaia Alves de Souza",
      "userId": "02292677914546630569"
     },
     "user_tz": 180
    },
    "id": "0F4J6dH4iFw5"
   },
   "outputs": [],
   "source": [
    "#Statistical Valuator\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXgIY1zCsLuz"
   },
   "source": [
    "### IMPORTING DATA AND RUNNING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 992,
     "status": "ok",
     "timestamp": 1754039229997,
     "user": {
      "displayName": "Pedro Isaia Alves de Souza",
      "userId": "02292677914546630569"
     },
     "user_tz": 180
    },
    "id": "cb6_Nv61sLnn"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', parse_dates=['date'])\n",
    "test = pd.read_csv('test.csv', parse_dates=['date'])\n",
    "oil = pd.read_csv('oil.csv', parse_dates=['date'])\n",
    "holidays = pd.read_csv('holidays_events.csv', parse_dates=['date'])\n",
    "transactions = pd.read_csv('transactions.csv', parse_dates=['date'])\n",
    "stores = pd.read_csv('stores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "error",
     "timestamp": 1754039230023,
     "user": {
      "displayName": "Pedro Isaia Alves de Souza",
      "userId": "02292677914546630569"
     },
     "user_tz": 180
    },
    "id": "P-_NS7T9sXuO",
    "outputId": "b9dfe7c2-fd13-425d-abc4-3de576964280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape after merges: (3029400, 13)\n"
     ]
    }
   ],
   "source": [
    "df = merge_dataframe(train, test, oil, holidays, transactions, stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1754039230029,
     "user": {
      "displayName": "Pedro Isaia Alves de Souza",
      "userId": "02292677914546630569"
     },
     "user_tz": 180
    },
    "id": "WEnvzlmFsX1I"
   },
   "outputs": [],
   "source": [
    "df = future_engineering(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 7319,
     "status": "aborted",
     "timestamp": 1754039230032,
     "user": {
      "displayName": "Pedro Isaia Alves de Souza",
      "userId": "02292677914546630569"
     },
     "user_tz": 180
    },
    "id": "TO7KmOHrwqsi"
   },
   "outputs": [],
   "source": [
    "model, X_val, y_val, X_test = train_model(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "516fbc79"
   },
   "source": [
    "### SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 7322,
     "status": "aborted",
     "timestamp": 1754039230035,
     "user": {
      "displayName": "Pedro Isaia Alves de Souza",
      "userId": "02292677914546630569"
     },
     "user_tz": 180
    },
    "id": "4dab1d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSLE: 1.2386956788053445\n"
     ]
    }
   ],
   "source": [
    "# Validation performance\n",
    "val_pred = model.predict(X_val)\n",
    "rmsle = np.sqrt(mean_squared_log_error(y_val, np.maximum(0, val_pred)))\n",
    "print(\"Validation RMSLE:\", rmsle)\n",
    "\n",
    "#Generate Submission\n",
    "# Predict on test set\n",
    "test['sales'] = np.maximum(0, model.predict(X_test))\n",
    "\n",
    "# Create submission\n",
    "submission = test[['id', 'sales']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1754039230039,
     "user": {
      "displayName": "Pedro Isaia Alves de Souza",
      "userId": "02292677914546630569"
     },
     "user_tz": 180
    },
    "id": "UPVDd406aTtk"
   },
   "outputs": [],
   "source": [
    "def merge_features(df, oil, holidays, transactions, stores):\n",
    "\n",
    "  # --- Merge Oil Prices ---\n",
    "  oil['dcoilwtico'] = oil['dcoilwtico'].ffill()\n",
    "  df = df.merge(oil, on='date', how='left')\n",
    "\n",
    "  # --- Merge Holidays ---\n",
    "  # Ensure 'date' column in holidays is datetime type before merging\n",
    "  holidays['date'] = pd.to_datetime(holidays['date'])\n",
    "  # Filter holidays to include only those present in the main dataframe's date range\n",
    "  holidays_filtered = holidays[holidays['date'].isin(df['date'].unique())]\n",
    "  # Create a column 'is_holiday' to indicate if a date is a holiday\n",
    "  holidays_filtered['is_holiday'] = 1\n",
    "  # Select only 'date' and 'is_holiday' columns and drop duplicates based on 'date'\n",
    "  holidays_processed = holidays_filtered[['date', 'is_holiday']].drop_duplicates(subset='date')\n",
    "  # Merge with the main dataframe\n",
    "  df = df.merge(holidays_processed, on='date', how='left')\n",
    "  # Fill NaN values in 'is_holiday' with 0, indicating non-holidays\n",
    "  df['is_holiday'] = df['is_holiday'].fillna(0)\n",
    "\n",
    "  # --- Merge Transactions ---\n",
    "  # Ensure 'date' column in transactions is datetime type before merging\n",
    "  transactions['date'] = pd.to_datetime(transactions['date'])\n",
    "  # Filter transactions to include only those present in the main dataframe's date and store_nbr combinations\n",
    "  transactions_filtered = transactions[transactions.set_index(['date', 'store_nbr']).index.isin(df.set_index(['date', 'store_nbr']).index)]\n",
    "  # Select only 'date', 'store_nbr', and 'transactions' columns and drop duplicates\n",
    "  transactions_processed = transactions_filtered[['date', 'store_nbr', 'transactions']].drop_duplicates(subset=['date', 'store_nbr'])\n",
    "  # Merge with the main dataframe\n",
    "  df = df.merge(transactions_processed, on=['date', 'store_nbr'], how='left')\n",
    "  # Fill NaN values in 'transactions' with 0\n",
    "  df['transactions'] = df['transactions'].fillna(0)\n",
    "\n",
    "\n",
    "  # --- Merge Store Metadata ---\n",
    "  # Select only 'store_nbr', 'city', 'state', and 'type' columns from the stores dataframe\n",
    "  stores_processed = stores[['store_nbr', 'city', 'state', 'type']].drop_duplicates(subset='store_nbr')\n",
    "  # Merge with the main dataframe\n",
    "  df = df.merge(stores_processed, on='store_nbr', how='left')\n",
    "\n",
    "\n",
    "  print(\"Dataset shape after merges:\", df.shape)\n",
    "\n",
    "  return df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNZI27TE0WJGmeqht2a+3NB",
   "collapsed_sections": [
    "tXgIY1zCsLuz",
    "516fbc79"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
>>>>>>> e164056 (add files .py)
